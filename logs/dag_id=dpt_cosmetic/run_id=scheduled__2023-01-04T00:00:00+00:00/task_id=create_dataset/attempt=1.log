[2023-01-05T17:58:30.549+0000] {taskinstance.py:1165} INFO - Dependencies all met for <TaskInstance: dpt_cosmetic.create_dataset scheduled__2023-01-04T00:00:00+00:00 [queued]>
[2023-01-05T17:58:30.561+0000] {taskinstance.py:1165} INFO - Dependencies all met for <TaskInstance: dpt_cosmetic.create_dataset scheduled__2023-01-04T00:00:00+00:00 [queued]>
[2023-01-05T17:58:30.562+0000] {taskinstance.py:1362} INFO - 
--------------------------------------------------------------------------------
[2023-01-05T17:58:30.563+0000] {taskinstance.py:1363} INFO - Starting attempt 1 of 1
[2023-01-05T17:58:30.563+0000] {taskinstance.py:1364} INFO - 
--------------------------------------------------------------------------------
[2023-01-05T17:58:30.582+0000] {taskinstance.py:1383} INFO - Executing <Task(GCSToBigQueryOperator): create_dataset> on 2023-01-04 00:00:00+00:00
[2023-01-05T17:58:30.588+0000] {standard_task_runner.py:55} INFO - Started process 59 to run task
[2023-01-05T17:58:30.592+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'dpt_cosmetic', 'create_dataset', 'scheduled__2023-01-04T00:00:00+00:00', '--job-id', '287', '--raw', '--subdir', 'DAGS_FOLDER/main.py', '--cfg-path', '/tmp/tmpwsp8newg']
[2023-01-05T17:58:30.595+0000] {standard_task_runner.py:83} INFO - Job 287: Subtask create_dataset
[2023-01-05T17:58:30.611+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.7/site-packages/***/configuration.py:545: DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
  option = self._get_environment_variables(deprecated_key, deprecated_section, key, section)

[2023-01-05T17:58:30.674+0000] {task_command.py:376} INFO - Running <TaskInstance: dpt_cosmetic.create_dataset scheduled__2023-01-04T00:00:00+00:00 [running]> on host 7dccb0a5668b
[2023-01-05T17:58:30.763+0000] {taskinstance.py:1592} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=dpt_cosmetic
AIRFLOW_CTX_TASK_ID=create_dataset
AIRFLOW_CTX_EXECUTION_DATE=2023-01-04T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2023-01-04T00:00:00+00:00
[2023-01-05T17:58:30.764+0000] {connection.py:429} ERROR - Unable to retrieve connection from secrets backend (EnvironmentVariablesBackend). Checking subsequent secrets backend.
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/connection.py", line 422, in get_connection_from_secrets
    conn = secrets_backend.get_connection(conn_id=conn_id)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/secrets/base_secrets.py", line 115, in get_connection
    return self.deserialize_connection(conn_id=conn_id, value=value)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/secrets/base_secrets.py", line 67, in deserialize_connection
    return Connection.from_json(conn_id=conn_id, value=value)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/connection.py", line 449, in from_json
    return Connection(conn_id=conn_id, **kwargs)
TypeError: __init__() got an unexpected keyword argument 'key_path'
[2023-01-05T17:58:30.777+0000] {base.py:71} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-05T17:58:30.788+0000] {bigquery.py:1554} INFO - Inserting job ***_1672941510787911_b181096cfe2dcbd1c285c96ce42c276f
[2023-01-05T17:58:31.590+0000] {taskinstance.py:1851} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 316, in execute
    description=self.description,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/hooks/bigquery.py", line 1816, in run_load
    job = self.insert_job(configuration=configuration, project_id=self.project_id)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/common/hooks/base_google.py", line 439, in inner_wrapper
    return func(self, *args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/hooks/bigquery.py", line 1560, in insert_job
    job.result(timeout=timeout, retry=retry)
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/job/base.py", line 725, in result
    self._begin(retry=retry, timeout=timeout)
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/job/base.py", line 518, in _begin
    timeout=timeout,
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/client.py", line 782, in _call_api
    return call()
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/retry.py", line 288, in retry_wrapped_func
    on_error=on_error,
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/retry.py", line 190, in retry_target
    return target()
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/_http/__init__.py", line 494, in api_request
    raise exceptions.from_http_response(response)
google.api_core.exceptions.BadRequest: 400 POST https://bigquery.googleapis.com/bigquery/v2/projects/my-kubeflow-370515/jobs?prettyPrint=false: Invalid project ID 'None'. Project IDs must contain 6-63 lowercase letters, digits, or dashes. Some project IDs also include domain name separated by a colon. IDs must start with a letter and may not end with a dash.
[2023-01-05T17:58:31.624+0000] {taskinstance.py:1406} INFO - Marking task as FAILED. dag_id=dpt_cosmetic, task_id=create_dataset, execution_date=20230104T000000, start_date=20230105T175830, end_date=20230105T175831
[2023-01-05T17:58:31.660+0000] {standard_task_runner.py:105} ERROR - Failed to execute job 287 for task create_dataset (400 POST https://bigquery.googleapis.com/bigquery/v2/projects/my-kubeflow-370515/jobs?prettyPrint=false: Invalid project ID 'None'. Project IDs must contain 6-63 lowercase letters, digits, or dashes. Some project IDs also include domain name separated by a colon. IDs must start with a letter and may not end with a dash.; 59)
[2023-01-05T17:58:31.691+0000] {local_task_job.py:164} INFO - Task exited with return code 1
[2023-01-05T17:58:31.746+0000] {local_task_job.py:273} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-05T18:09:20.750+0000] {taskinstance.py:1165} INFO - Dependencies all met for <TaskInstance: dpt_cosmetic.create_dataset scheduled__2023-01-04T00:00:00+00:00 [queued]>
[2023-01-05T18:09:20.766+0000] {taskinstance.py:1165} INFO - Dependencies all met for <TaskInstance: dpt_cosmetic.create_dataset scheduled__2023-01-04T00:00:00+00:00 [queued]>
[2023-01-05T18:09:20.767+0000] {taskinstance.py:1362} INFO - 
--------------------------------------------------------------------------------
[2023-01-05T18:09:20.767+0000] {taskinstance.py:1363} INFO - Starting attempt 1 of 1
[2023-01-05T18:09:20.768+0000] {taskinstance.py:1364} INFO - 
--------------------------------------------------------------------------------
[2023-01-05T18:09:20.792+0000] {taskinstance.py:1383} INFO - Executing <Task(GCSToBigQueryOperator): create_dataset> on 2023-01-04 00:00:00+00:00
[2023-01-05T18:09:20.799+0000] {standard_task_runner.py:55} INFO - Started process 77 to run task
[2023-01-05T18:09:20.804+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'dpt_cosmetic', 'create_dataset', 'scheduled__2023-01-04T00:00:00+00:00', '--job-id', '293', '--raw', '--subdir', 'DAGS_FOLDER/main.py', '--cfg-path', '/tmp/tmp1umu9iki']
[2023-01-05T18:09:20.807+0000] {standard_task_runner.py:83} INFO - Job 293: Subtask create_dataset
[2023-01-05T18:09:20.824+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.7/site-packages/***/configuration.py:545: DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
  option = self._get_environment_variables(deprecated_key, deprecated_section, key, section)

[2023-01-05T18:09:20.917+0000] {task_command.py:376} INFO - Running <TaskInstance: dpt_cosmetic.create_dataset scheduled__2023-01-04T00:00:00+00:00 [running]> on host 7dccb0a5668b
[2023-01-05T18:09:21.006+0000] {taskinstance.py:1592} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=dpt_cosmetic
AIRFLOW_CTX_TASK_ID=create_dataset
AIRFLOW_CTX_EXECUTION_DATE=2023-01-04T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2023-01-04T00:00:00+00:00
[2023-01-05T18:09:21.008+0000] {connection.py:429} ERROR - Unable to retrieve connection from secrets backend (EnvironmentVariablesBackend). Checking subsequent secrets backend.
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/connection.py", line 422, in get_connection_from_secrets
    conn = secrets_backend.get_connection(conn_id=conn_id)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/secrets/base_secrets.py", line 115, in get_connection
    return self.deserialize_connection(conn_id=conn_id, value=value)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/secrets/base_secrets.py", line 67, in deserialize_connection
    return Connection.from_json(conn_id=conn_id, value=value)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/connection.py", line 449, in from_json
    return Connection(conn_id=conn_id, **kwargs)
TypeError: __init__() got an unexpected keyword argument 'key_path'
[2023-01-05T18:09:21.020+0000] {base.py:71} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-05T18:09:21.032+0000] {bigquery.py:2842} INFO - Project not included in destination_project_dataset_table: cosmetic.user; using project "my-kubeflow-370515"
[2023-01-05T18:09:21.034+0000] {bigquery.py:1554} INFO - Inserting job ***_1672942161033873_629c435df5d065e010053f6bcf337122
[2023-01-05T18:09:21.976+0000] {taskinstance.py:1851} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 316, in execute
    description=self.description,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/hooks/bigquery.py", line 1816, in run_load
    job = self.insert_job(configuration=configuration, project_id=self.project_id)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/common/hooks/base_google.py", line 439, in inner_wrapper
    return func(self, *args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/hooks/bigquery.py", line 1560, in insert_job
    job.result(timeout=timeout, retry=retry)
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/bigquery/job/base.py", line 728, in result
    return super(_AsyncJob, self).result(timeout=timeout, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/google/api_core/future/polling.py", line 137, in result
    raise self._exception
google.api_core.exceptions.NotFound: 404 Not found: URI gs://None
[2023-01-05T18:09:21.996+0000] {taskinstance.py:1406} INFO - Marking task as FAILED. dag_id=dpt_cosmetic, task_id=create_dataset, execution_date=20230104T000000, start_date=20230105T180920, end_date=20230105T180921
[2023-01-05T18:09:22.027+0000] {standard_task_runner.py:105} ERROR - Failed to execute job 293 for task create_dataset (404 Not found: URI gs://None; 77)
[2023-01-05T18:09:22.064+0000] {local_task_job.py:164} INFO - Task exited with return code 1
[2023-01-05T18:09:22.119+0000] {local_task_job.py:273} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-05T22:48:34.699+0000] {taskinstance.py:1165} INFO - Dependencies all met for <TaskInstance: dpt_cosmetic.create_dataset scheduled__2023-01-04T00:00:00+00:00 [queued]>
[2023-01-05T22:48:34.771+0000] {taskinstance.py:1165} INFO - Dependencies all met for <TaskInstance: dpt_cosmetic.create_dataset scheduled__2023-01-04T00:00:00+00:00 [queued]>
[2023-01-05T22:48:34.773+0000] {taskinstance.py:1362} INFO - 
--------------------------------------------------------------------------------
[2023-01-05T22:48:34.774+0000] {taskinstance.py:1363} INFO - Starting attempt 1 of 1
[2023-01-05T22:48:34.775+0000] {taskinstance.py:1364} INFO - 
--------------------------------------------------------------------------------
[2023-01-05T22:48:34.870+0000] {taskinstance.py:1383} INFO - Executing <Task(GCSToBigQueryOperator): create_dataset> on 2023-01-04 00:00:00+00:00
[2023-01-05T22:48:34.894+0000] {standard_task_runner.py:55} INFO - Started process 126 to run task
[2023-01-05T22:48:34.912+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'dpt_cosmetic', 'create_dataset', 'scheduled__2023-01-04T00:00:00+00:00', '--job-id', '321', '--raw', '--subdir', 'DAGS_FOLDER/main.py', '--cfg-path', '/tmp/tmpz5qsgoij']
[2023-01-05T22:48:34.923+0000] {standard_task_runner.py:83} INFO - Job 321: Subtask create_dataset
[2023-01-05T22:48:34.980+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.7/site-packages/***/configuration.py:545: DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
  option = self._get_environment_variables(deprecated_key, deprecated_section, key, section)

[2023-01-05T22:48:35.232+0000] {task_command.py:376} INFO - Running <TaskInstance: dpt_cosmetic.create_dataset scheduled__2023-01-04T00:00:00+00:00 [running]> on host fdf4acf1d8ed
[2023-01-05T22:48:35.602+0000] {taskinstance.py:1592} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=dpt_cosmetic
AIRFLOW_CTX_TASK_ID=create_dataset
AIRFLOW_CTX_EXECUTION_DATE=2023-01-04T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2023-01-04T00:00:00+00:00
[2023-01-05T22:48:35.607+0000] {connection.py:429} ERROR - Unable to retrieve connection from secrets backend (EnvironmentVariablesBackend). Checking subsequent secrets backend.
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/connection.py", line 422, in get_connection_from_secrets
    conn = secrets_backend.get_connection(conn_id=conn_id)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/secrets/base_secrets.py", line 115, in get_connection
    return self.deserialize_connection(conn_id=conn_id, value=value)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/secrets/base_secrets.py", line 67, in deserialize_connection
    return Connection.from_json(conn_id=conn_id, value=value)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/connection.py", line 449, in from_json
    return Connection(conn_id=conn_id, **kwargs)
TypeError: __init__() got an unexpected keyword argument 'key_path'
[2023-01-05T22:48:35.647+0000] {base.py:71} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-05T22:48:35.651+0000] {connection.py:429} ERROR - Unable to retrieve connection from secrets backend (EnvironmentVariablesBackend). Checking subsequent secrets backend.
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/connection.py", line 422, in get_connection_from_secrets
    conn = secrets_backend.get_connection(conn_id=conn_id)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/secrets/base_secrets.py", line 115, in get_connection
    return self.deserialize_connection(conn_id=conn_id, value=value)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/secrets/base_secrets.py", line 67, in deserialize_connection
    return Connection.from_json(conn_id=conn_id, value=value)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/connection.py", line 449, in from_json
    return Connection(conn_id=conn_id, **kwargs)
TypeError: __init__() got an unexpected keyword argument 'key_path'
[2023-01-05T22:48:35.676+0000] {base.py:71} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-05T22:48:36.280+0000] {gcs.py:340} ERROR - Download attempt of object: gs://***_test_sd2beatles/schema/user.json from gs://***_test_sd2beatles/schema/user.json has failed. Attempt: 1, max 1.
[2023-01-05T22:48:36.283+0000] {taskinstance.py:1851} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/storage/client.py", line 1123, in download_blob_to_file
    retry=retry,
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/storage/blob.py", line 1003, in _do_download
    response = download.consume(transport, timeout=timeout)
  File "/home/airflow/.local/lib/python3.7/site-packages/google/resumable_media/requests/download.py", line 233, in consume
    retriable_request, self._get_status_code, self._retry_strategy
  File "/home/airflow/.local/lib/python3.7/site-packages/google/resumable_media/requests/_request_helpers.py", line 148, in wait_and_retry
    response = func()
  File "/home/airflow/.local/lib/python3.7/site-packages/google/resumable_media/requests/download.py", line 214, in retriable_request
    self._process_response(result)
  File "/home/airflow/.local/lib/python3.7/site-packages/google/resumable_media/_download.py", line 189, in _process_response
    response, _ACCEPTABLE_STATUS_CODES, self._get_status_code
  File "/home/airflow/.local/lib/python3.7/site-packages/google/resumable_media/_helpers.py", line 113, in require_status_code
    *status_codes
google.resumable_media.common.InvalidResponse: ('Request failed with status code', 404, 'Expected one of', <HTTPStatus.OK: 200>, <HTTPStatus.PARTIAL_CONTENT: 206>)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 255, in execute
    object_name=self.schema_object,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/google/cloud/hooks/gcs.py", line 331, in download
    return blob.download_as_bytes()
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/storage/blob.py", line 1431, in download_as_bytes
    retry=retry,
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/storage/client.py", line 1126, in download_blob_to_file
    _raise_from_invalid_response(exc)
  File "/home/airflow/.local/lib/python3.7/site-packages/google/cloud/storage/blob.py", line 4470, in _raise_from_invalid_response
    raise exceptions.from_http_status(response.status_code, message, response=response)
google.api_core.exceptions.NotFound: 404 GET https://storage.googleapis.com/download/storage/v1/b/airflow_test_sd2beatles/o/gs%3A%2F%2Fairflow_test_sd2beatles%2Fschema%2Fuser.json?alt=media: No such object: airflow_test_sd2beatles/gs://airflow_test_sd2beatles/schema/user.json: ('Request failed with status code', 404, 'Expected one of', <HTTPStatus.OK: 200>, <HTTPStatus.PARTIAL_CONTENT: 206>)
[2023-01-05T22:48:36.324+0000] {taskinstance.py:1406} INFO - Marking task as FAILED. dag_id=dpt_cosmetic, task_id=create_dataset, execution_date=20230104T000000, start_date=20230105T224834, end_date=20230105T224836
[2023-01-05T22:48:36.378+0000] {standard_task_runner.py:105} ERROR - Failed to execute job 321 for task create_dataset (404 GET https://storage.googleapis.com/download/storage/v1/b/airflow_test_sd2beatles/o/gs%3A%2F%2Fairflow_test_sd2beatles%2Fschema%2Fuser.json?alt=media: No such object: airflow_test_sd2beatles/gs://airflow_test_sd2beatles/schema/user.json: ('Request failed with status code', 404, 'Expected one of', <HTTPStatus.OK: 200>, <HTTPStatus.PARTIAL_CONTENT: 206>); 126)
[2023-01-05T22:48:36.411+0000] {local_task_job.py:164} INFO - Task exited with return code 1
[2023-01-05T22:48:36.506+0000] {local_task_job.py:273} INFO - 0 downstream tasks scheduled from follow-on schedule check
